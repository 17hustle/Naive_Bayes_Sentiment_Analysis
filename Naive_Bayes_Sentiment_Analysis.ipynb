{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-26T05:50:50.185747Z","iopub.execute_input":"2025-02-26T05:50:50.186078Z","iopub.status.idle":"2025-02-26T05:50:50.506649Z","shell.execute_reply.started":"2025-02-26T05:50:50.186049Z","shell.execute_reply":"2025-02-26T05:50:50.505808Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/twitter-entity-sentiment-analysis/twitter_validation.csv\n/kaggle/input/twitter-entity-sentiment-analysis/twitter_training.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nimport pdb\nfrom nltk.corpus import stopwords, twitter_samples\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport string\nfrom nltk.tokenize import TweetTokenizer\nfrom os import getcwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T05:50:52.616849Z","iopub.execute_input":"2025-02-26T05:50:52.617142Z","iopub.status.idle":"2025-02-26T05:50:53.670534Z","shell.execute_reply.started":"2025-02-26T05:50:52.617117Z","shell.execute_reply":"2025-02-26T05:50:53.669867Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"nltk.download('twitter_samples')\nnltk.download('stopwords')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T05:50:54.946962Z","iopub.execute_input":"2025-02-26T05:50:54.947381Z","iopub.status.idle":"2025-02-26T05:50:55.168755Z","shell.execute_reply.started":"2025-02-26T05:50:54.947355Z","shell.execute_reply":"2025-02-26T05:50:55.168083Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"all_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T05:50:56.353633Z","iopub.execute_input":"2025-02-26T05:50:56.353913Z","iopub.status.idle":"2025-02-26T05:50:56.923997Z","shell.execute_reply.started":"2025-02-26T05:50:56.353891Z","shell.execute_reply":"2025-02-26T05:50:56.923371Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"all_positive_tweets[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T05:50:56.925028Z","iopub.execute_input":"2025-02-26T05:50:56.925228Z","iopub.status.idle":"2025-02-26T05:50:56.930247Z","shell.execute_reply.started":"2025-02-26T05:50:56.925211Z","shell.execute_reply":"2025-02-26T05:50:56.929557Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)'"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"#splitting the dataset\ntest_pos = all_positive_tweets[4000:]\ntrain_pos = all_positive_tweets[:4000]\ntest_neg = all_negative_tweets[4000:]\ntrain_neg = all_negative_tweets[:4000]\n\n\ntrain_x = train_pos + train_neg\ntest_x = test_pos + test_neg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T05:50:56.931492Z","iopub.execute_input":"2025-02-26T05:50:56.931740Z","iopub.status.idle":"2025-02-26T05:50:56.941191Z","shell.execute_reply.started":"2025-02-26T05:50:56.931721Z","shell.execute_reply":"2025-02-26T05:50:56.940354Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"len(train_x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T05:50:58.383211Z","iopub.execute_input":"2025-02-26T05:50:58.383570Z","iopub.status.idle":"2025-02-26T05:50:58.388311Z","shell.execute_reply.started":"2025-02-26T05:50:58.383542Z","shell.execute_reply":"2025-02-26T05:50:58.387538Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"8000"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"len(test_x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T05:51:00.450822Z","iopub.execute_input":"2025-02-26T05:51:00.451094Z","iopub.status.idle":"2025-02-26T05:51:00.455747Z","shell.execute_reply.started":"2025-02-26T05:51:00.451074Z","shell.execute_reply":"2025-02-26T05:51:00.455031Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"2000"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"#Creating train and test y\ntrain_y = np.append(np.ones(len(train_pos)),np.zeros(len(train_neg)))\ntest_y = np.append(np.ones(len(test_pos)),np.zeros(len(test_neg)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T05:51:01.469034Z","iopub.execute_input":"2025-02-26T05:51:01.469341Z","iopub.status.idle":"2025-02-26T05:51:01.473738Z","shell.execute_reply.started":"2025-02-26T05:51:01.469315Z","shell.execute_reply":"2025-02-26T05:51:01.473027Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Processing the data","metadata":{}},{"cell_type":"code","source":"import re\nimport string\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\n\ndef process_tweet(tweet):\n    \"\"\"\n    Preprocesses a tweet by:\n    - Lowercasing\n    - Removing URLs\n    - Removing handles (@username)\n    - Removing punctuation\n    - Tokenizing\n    - Removing stopwords\n    - Removing short words (length < 2)\n    \n    Input:\n        tweet: a string containing a tweet\n    Output:\n        cleaned_words: a list of processed words from the tweet\n    \"\"\"\n    # Convert to lowercase\n    tweet = tweet.lower()\n    \n    # Remove URLs\n    tweet = re.sub(r'http\\S+|www\\S+', '', tweet)\n    \n    # Remove mentions (@username)\n    tweet = re.sub(r'@\\w+', '', tweet)\n    \n    # Remove punctuations\n    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n    \n    # Tokenize tweet\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n    words = tokenizer.tokenize(tweet)\n    \n    # Remove stopwords and short words\n    stop_words = set(stopwords.words('english'))\n    cleaned_words = [word for word in words if word not in stop_words and len(word) > 1]\n    \n    return cleaned_words\n\n# Example usage\ntweet = \"@user I love NLP! ðŸ˜ Check out https://nlp.com\"\nprint(process_tweet(tweet))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T05:51:04.918871Z","iopub.execute_input":"2025-02-26T05:51:04.919189Z","iopub.status.idle":"2025-02-26T05:51:04.928967Z","shell.execute_reply.started":"2025-02-26T05:51:04.919169Z","shell.execute_reply":"2025-02-26T05:51:04.928180Z"}},"outputs":[{"name":"stdout","text":"['love', 'nlp', 'check']\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Count Tweets","metadata":{}},{"cell_type":"code","source":"def count_tweets(result, tweets, ys):\n    #result - a dict that will map each pair to its freq or occurence\n    # tweets - list of tweets we have\n    # ys: list of corresponding sentiment of each tweet\n\n    for y, tweet in zip(ys,tweets):\n        for word in process_tweet(tweet):\n            pair=(word,y)\n            if pair in result:\n                result[pair]+=1 #already present so increase by 1\n\n            else:\n                result[pair] =1 #present for the first time so turn it to 1\n\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T05:51:09.987339Z","iopub.execute_input":"2025-02-26T05:51:09.987613Z","iopub.status.idle":"2025-02-26T05:51:09.991725Z","shell.execute_reply.started":"2025-02-26T05:51:09.987592Z","shell.execute_reply":"2025-02-26T05:51:09.990781Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Model training using Naive Bayes\n\n* First part is to identify the number of classes we have\n* Create probability for each class","metadata":{}},{"cell_type":"code","source":"freqs = count_tweets({}, train_x, train_y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T05:51:13.788302Z","iopub.execute_input":"2025-02-26T05:51:13.788616Z","iopub.status.idle":"2025-02-26T05:51:15.421250Z","shell.execute_reply.started":"2025-02-26T05:51:13.788589Z","shell.execute_reply":"2025-02-26T05:51:15.420373Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Training NB\n* Given a freqs dictionary, train_x (a list of tweets) and a train_y (a list of labels for each tweet), implement a naive bayes classifier.\n* Calculate V\n* Calculate freq of pos and negative\n* Using freqs dict we can also compute the total number of pos and neg words\n* Using train_y we can compute the total number of documents (D), D_pos and D_neg as well","metadata":{}},{"cell_type":"code","source":"def train_naive_bayes(freqs, train_x, train_y):\n    #freqs : a dict that has (word, label) to find how often the word appears\n    #train_x: list of tweets\n    # train_y : lsit of labels (0,1) corresponding to tweets\n    loglikelihood = {}\n    logprior = 0\n    vocab = set(pair[0] for pair in freqs.keys())\n    V = len(vocab)\n\n\n    N_pos = N_neg = 0\n    for pair in freqs.keys():\n        if pair[1] > 0:\n            N_pos += freqs[pair]\n        else:\n            N_neg += freqs[pair]\n\n    D = len(train_y)\n    D_pos = sum(train_y)\n    D_neg = D-D_pos\n\n    logprior = np.log(D_pos) - np.log(D_neg)\n\n    for word in vocab:\n        freq_pos = freqs.get((word,1), 0)\n        freq_neg = freqs.get((word,0), 0)\n\n        #Calculating the probability that each word is pos or neg\n        p_w_pos = (freq_pos+1)/(N_pos + V)\n        p_w_neg = (freq_neg + 1)/(N_neg + V)\n\n        loglikelihood[word] =  np.log(p_w_pos) - np.log(p_w_neg)\n\n    return logprior, loglikelihood\n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:02:18.510552Z","iopub.execute_input":"2025-02-26T06:02:18.510860Z","iopub.status.idle":"2025-02-26T06:02:18.516680Z","shell.execute_reply.started":"2025-02-26T06:02:18.510838Z","shell.execute_reply":"2025-02-26T06:02:18.515846Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\nprint(logprior)\nprint(len(loglikelihood))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:02:33.602155Z","iopub.execute_input":"2025-02-26T06:02:33.602485Z","iopub.status.idle":"2025-02-26T06:02:33.638974Z","shell.execute_reply.started":"2025-02-26T06:02:33.602457Z","shell.execute_reply":"2025-02-26T06:02:33.638030Z"}},"outputs":[{"name":"stdout","text":"0.0\n10620\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"def naive_bayes_predict(tweet, logprior, loglikelihood):\n    word_1 = process_tweet(tweet)\n    p = 0\n    p+= logprior\n    for w in word_1:\n        if w in loglikelihood:\n            p+=loglikelihood[w]\n    return p","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:08:56.884748Z","iopub.execute_input":"2025-02-26T06:08:56.885020Z","iopub.status.idle":"2025-02-26T06:08:56.889463Z","shell.execute_reply.started":"2025-02-26T06:08:56.884999Z","shell.execute_reply":"2025-02-26T06:08:56.888656Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"my_tweet = 'He laughed.'\np = naive_bayes_predict(my_tweet, logprior, loglikelihood)\nprint('The expected output is', p)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:08:57.072233Z","iopub.execute_input":"2025-02-26T06:08:57.072502Z","iopub.status.idle":"2025-02-26T06:08:57.077410Z","shell.execute_reply.started":"2025-02-26T06:08:57.072481Z","shell.execute_reply":"2025-02-26T06:08:57.076616Z"}},"outputs":[{"name":"stdout","text":"The expected output is -1.114712842685476\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Test Naive Bayes to check accuracy","metadata":{}},{"cell_type":"code","source":"def test_naive_bayes(test_x, test_y, logprior, loglikelihood, naieve_bayes_predict=naive_bayes_predict):\n    accuracy = 0\n    y_hats = []\n    if naive_bayes_predict(tweet, logprior, loglikelihood)>0:\n        y_hat_i = 1\n    else:\n        y_hat_i = 0\n    y_hats.append(y_hat_i)\n\n    error = sum(abs(y_hat- y_true) for y_hat, y_true in zip(y_hats,test_y))/len(test_y)\n    accuracy = 1- error\n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:14:24.263703Z","iopub.execute_input":"2025-02-26T06:14:24.263981Z","iopub.status.idle":"2025-02-26T06:14:24.268524Z","shell.execute_reply.started":"2025-02-26T06:14:24.263959Z","shell.execute_reply":"2025-02-26T06:14:24.267683Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Run this cell to test your function\nfor tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n    # print( '%s -> %f' % (tweet, naive_bayes_predict(tweet, logprior, loglikelihood)))\n    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n#     print(f'{tweet} -> {p:.2f} ({p_category})')\n    print(f'{tweet} -> {p:.2f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:14:47.032222Z","iopub.execute_input":"2025-02-26T06:14:47.032537Z","iopub.status.idle":"2025-02-26T06:14:47.040874Z","shell.execute_reply.started":"2025-02-26T06:14:47.032513Z","shell.execute_reply":"2025-02-26T06:14:47.040028Z"}},"outputs":[{"name":"stdout","text":"I am happy -> 2.05\nI am bad -> -1.28\nthis movie should have been great. -> 2.03\ngreat -> 2.19\ngreat great -> 4.38\ngreat great great -> 6.57\ngreat great great great -> 8.75\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# Filtering words by ratio of pos and neg counts\n* Some words have more pos counts than others -> \"more postitve\"\n* Some words have mroe neg counts that others -> \"more negative\"\n* It can be determined using the loglikelihood calculations to determine the relative pos and neg of words","metadata":{}},{"cell_type":"code","source":"def get_ratio(freqs, word):\n    pos_neg_ratio = {\n        'positive':0,\n        'negative': 0,\n        'ratio':0.0\n    }\n    pos_neg_ratio['positive']=freqs.get((word,1),0)\n    pos_neg_ratio['negative']=freqs.get((word,0),0)\n    pos_neg_ratio['ratio']=(pos_neg_ratio['positive']+1)/(pos_neg_ratio['negative']+1) \n    # +1 to handle division by - 0 edge/error case\n\n    return pos_neg_ratio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:29:33.048515Z","iopub.execute_input":"2025-02-26T06:29:33.048825Z","iopub.status.idle":"2025-02-26T06:29:33.053402Z","shell.execute_reply.started":"2025-02-26T06:29:33.048802Z","shell.execute_reply":"2025-02-26T06:29:33.052354Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"# Get words by threshold\n* The function extracts words from freqs that meet the given threshold condition.\n* It returns a dictionary containing only words that meet a specified threshold condition.","metadata":{}},{"cell_type":"code","source":"def get_words_by_threshold(freqs,label,threshold,get_ratio=get_ratio):\n    word_list = {}\n    for key in freqs.keys():\n        w,_ = key\n        pos_neg_ratio = get_ratio(freqs,w)\n        if label == 1 and pos_neg_raio['ratio']>=threshold:\n            word_list[w] = pos_neg_ratio\n\n        \n        elif label == 0 and pos_neg_ratio['ratio']<=threshold:\n            word_list[w] = pos_neg_ratio\n    return word_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:33:08.983312Z","iopub.execute_input":"2025-02-26T06:33:08.983597Z","iopub.status.idle":"2025-02-26T06:33:08.987743Z","shell.execute_reply.started":"2025-02-26T06:33:08.983575Z","shell.execute_reply":"2025-02-26T06:33:08.986801Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"# Predict your own tweet!","metadata":{}},{"cell_type":"code","source":"\nmy_tweet = 'I am happy because I am moving out :)'\n\np = naive_bayes_predict(my_tweet, logprior, loglikelihood)\nprint(p)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:34:49.108444Z","iopub.execute_input":"2025-02-26T06:34:49.108712Z","iopub.status.idle":"2025-02-26T06:34:49.113410Z","shell.execute_reply.started":"2025-02-26T06:34:49.108692Z","shell.execute_reply":"2025-02-26T06:34:49.112691Z"}},"outputs":[{"name":"stdout","text":"0.9353829182269733\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"my_tweet = 'I am sad because I am sick.'\n\np = naive_bayes_predict(my_tweet, logprior, loglikelihood)\nprint(p)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:35:06.534390Z","iopub.execute_input":"2025-02-26T06:35:06.534701Z","iopub.status.idle":"2025-02-26T06:35:06.539539Z","shell.execute_reply.started":"2025-02-26T06:35:06.534676Z","shell.execute_reply":"2025-02-26T06:35:06.538869Z"}},"outputs":[{"name":"stdout","text":"-5.800834555010549\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}